{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_i4V7kWEWf6",
        "outputId": "b7220c33-f069-4a69-a8fb-cfe23065b843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('gemini_key')"
      ],
      "metadata": {
        "id": "CPKAjBEoE3ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "HYPj9jT6SR5w",
        "outputId": "f9b6f99d-2dd0-49eb-a656-6063ddb2331c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-flash-preview\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/deep-research-pro-preview-12-2025\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
            "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "0kcX1As0IdV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_templates=\"\"\"\n",
        "proble : Language detection\n",
        "Content : you are helping a translation system detect text language\n",
        "role : you are a language detection AI assistant\n",
        "constraints :\n",
        "1. No explanation\n",
        "2. Input is one sentence\n",
        "3. Output must be exactly one word\n",
        "\n",
        "Instructions:\n",
        "1. Read the text\n",
        "2. Identify language\n",
        "3. Respond with the language name only\n",
        "Input:\n",
        "{text}\n",
        "Output:\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "YMq11ltOI13e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.list_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Gbw03nOJJE",
        "outputId": "1a9b38c7-da8c-4ced-d4a7-9959a29c2cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object list_models at 0x7f71601c33d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(genai.list_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6uj6PbVVOUHx",
        "outputId": "ef7a275e-9ffe-4d77-a8d0-4397417181ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/gemini-2.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 2.5 Flash',\n",
              "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
              "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-pro',\n",
              "       base_model_id='',\n",
              "       version='2.5',\n",
              "       display_name='Gemini 2.5 Pro',\n",
              "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.0-flash',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash',\n",
              "       description='Gemini 2.0 Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-001',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash 001',\n",
              "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
              "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
              "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-lite-001',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash-Lite 001',\n",
              "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-2.0-flash-lite',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Gemini 2.0 Flash-Lite',\n",
              "       description='Gemini 2.0 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/gemini-exp-1206',\n",
              "       base_model_id='',\n",
              "       version='2.5-exp-03-25',\n",
              "       display_name='Gemini Experimental 1206',\n",
              "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-preview-tts',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
              "       display_name='Gemini 2.5 Flash Preview TTS',\n",
              "       description='Gemini 2.5 Flash Preview TTS',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=16384,\n",
              "       supported_generation_methods=['countTokens', 'generateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-pro-preview-tts',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
              "       display_name='Gemini 2.5 Pro Preview TTS',\n",
              "       description='Gemini 2.5 Pro Preview TTS',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=16384,\n",
              "       supported_generation_methods=['countTokens', 'generateContent', 'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-1b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 1B',\n",
              "       description='',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-4b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 4B',\n",
              "       description='',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-12b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 12B',\n",
              "       description='',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3-27b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3 27B',\n",
              "       description='',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3n-e4b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3n E4B',\n",
              "       description='',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemma-3n-e2b-it',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemma 3n E2B',\n",
              "       description='',\n",
              "       input_token_limit=8192,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=None,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini Flash Latest',\n",
              "       display_name='Gemini Flash Latest',\n",
              "       description='Latest release of Gemini Flash',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-flash-lite-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini Flash-Lite Latest',\n",
              "       display_name='Gemini Flash-Lite Latest',\n",
              "       description='Latest release of Gemini Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini Pro Latest',\n",
              "       display_name='Gemini Pro Latest',\n",
              "       description='Latest release of Gemini Pro',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-lite',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 2.5 Flash-Lite',\n",
              "       description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-image',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Nano Banana',\n",
              "       description='Gemini 2.5 Flash Preview Image',\n",
              "       input_token_limit=32768,\n",
              "       output_token_limit=32768,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
              "       base_model_id='',\n",
              "       version='Gemini 2.5 Flash Preview 09-2025',\n",
              "       display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
              "       description='Gemini 2.5 Flash Preview Sep 2025',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
              "       base_model_id='',\n",
              "       version='2.5-preview-09-25',\n",
              "       display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
              "       description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-3-pro-preview',\n",
              "       base_model_id='',\n",
              "       version='3-pro-preview-11-2025',\n",
              "       display_name='Gemini 3 Pro Preview',\n",
              "       description='Gemini 3 Pro Preview',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-3-flash-preview',\n",
              "       base_model_id='',\n",
              "       version='3-flash-preview-12-2025',\n",
              "       display_name='Gemini 3 Flash Preview',\n",
              "       description='Gemini 3 Flash Preview',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent',\n",
              "                                     'countTokens',\n",
              "                                     'createCachedContent',\n",
              "                                     'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-3-pro-image-preview',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Nano Banana Pro',\n",
              "       description='Gemini 3 Pro Image Preview',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=32768,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/nano-banana-pro-preview',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Nano Banana Pro',\n",
              "       description='Gemini 3 Pro Image Preview',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=32768,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-robotics-er-1.5-preview',\n",
              "       base_model_id='',\n",
              "       version='1.5-preview',\n",
              "       display_name='Gemini Robotics-ER 1.5 Preview',\n",
              "       description='Gemini Robotics-ER 1.5 Preview',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
              "       base_model_id='',\n",
              "       version='Gemini 2.5 Computer Use Preview 10-2025',\n",
              "       display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
              "       description='Gemini 2.5 Computer Use Preview 10-2025',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/deep-research-pro-preview-12-2025',\n",
              "       base_model_id='',\n",
              "       version='deepthink-exp-05-20',\n",
              "       display_name='Deep Research Pro Preview (Dec-12-2025)',\n",
              "       description='Preview release (December 12th, 2025) of Deep Research Pro',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=65536,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       max_temperature=None,\n",
              "       top_p=1.0,\n",
              "       top_k=40),\n",
              " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
              "       base_model_id='',\n",
              "       version='01',\n",
              "       display_name='Imagen 4 (Preview)',\n",
              "       description='Vertex served Imagen 4.0 model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
              "       base_model_id='',\n",
              "       version='01',\n",
              "       display_name='Imagen 4 Ultra (Preview)',\n",
              "       description='Vertex served Imagen 4.0 ultra model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-generate-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Imagen 4',\n",
              "       description='Vertex served Imagen 4.0 model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-ultra-generate-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Imagen 4 Ultra',\n",
              "       description='Vertex served Imagen 4.0 ultra model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/imagen-4.0-fast-generate-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Imagen 4 Fast',\n",
              "       description='Vertex served Imagen 4.0 Fast model',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predict'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-2.0-generate-001',\n",
              "       base_model_id='',\n",
              "       version='2.0',\n",
              "       display_name='Veo 2',\n",
              "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
              "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
              "                    'https://console.cloud.google.com/billing to enable it.'),\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.0-generate-001',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Veo 3',\n",
              "       description='Veo 3',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.0-fast-generate-001',\n",
              "       base_model_id='',\n",
              "       version='3.0',\n",
              "       display_name='Veo 3 fast',\n",
              "       description='Veo 3 fast',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.1-generate-preview',\n",
              "       base_model_id='',\n",
              "       version='3.1',\n",
              "       display_name='Veo 3.1',\n",
              "       description='Veo 3.1',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/veo-3.1-fast-generate-preview',\n",
              "       base_model_id='',\n",
              "       version='3.1',\n",
              "       display_name='Veo 3.1 fast',\n",
              "       description='Veo 3.1 fast',\n",
              "       input_token_limit=480,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['predictLongRunning'],\n",
              "       temperature=None,\n",
              "       max_temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
              "       base_model_id='',\n",
              "       version='Gemini 2.5 Flash Native Audio Latest',\n",
              "       display_name='Gemini 2.5 Flash Native Audio Latest',\n",
              "       description='Latest release of Gemini 2.5 Flash Native Audio',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
              "       base_model_id='',\n",
              "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
              "       display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
              "       description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-2.5-flash-native-audio-preview-12-2025',\n",
              "       base_model_id='',\n",
              "       version='12-2025',\n",
              "       display_name='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
              "       description='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
              "       input_token_limit=131072,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
              "       temperature=1.0,\n",
              "       max_temperature=2.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel('gemini-flash-latest')"
      ],
      "metadata": {
        "id": "xh2y8SsqOZ4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generate_content('prompt_templates')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "53SxtAMxOvW6",
        "outputId": "abbccb57-e32a-4b07-f03e-b73643c000a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"Prompt templates are predefined structures used to guide Large Language Models (LLMs) like ChatGPT, Claude, or Llama to generate consistent, high-quality outputs. Think of them as **\\\"Mad Libs\\\" for AI.**\\n\\nBy using templates, you can swap out specific variables while keeping the core instructions and formatting the same.\\n\\n---\\n\\n### 1. The Anatomy of a Prompt Template\\nA good template usually consists of four main components:\\n1.  **Role/Persona:** Who the AI should be (e.g., \\\"You are a Senior Python Developer\\\").\\n2.  **Instruction:** What the AI should do (e.g., \\\"Refactor this code\\\").\\n3.  **Context/Constraints:** Rules to follow (e.g., \\\"Do not use external libraries\\\").\\n4.  **Placeholders/Variables:** The dynamic parts, usually in brackets like `{text}` or `[TOPIC]`.\\n\\n---\\n\\n### 2. Common Examples\\n\\n#### A. Content Summarization\\n> \\\"Act as a professional editor. Summarize the following text provided below. The summary should be no longer than **{word_count}** words and should be written for a **{target_audience}** audience.\\n> \\n> **Text:** {input_text}\\\"\\n\\n#### B. Technical Support/Coding\\n> \\\"You are an expert in **{programming_language}**. Please debug the following code snippet. Explain why the error is happening and provide the corrected code using **{style_preference}** (e.g., PEP8).\\n> \\n> **Code:** {code_snippet}\\\"\\n\\n#### C. Role-play/Learning\\n> \\\"You are a world-class **{expert_type}**. I am a beginner trying to understand **{topic}**. Explain this concept to me using a simple analogy and avoid using technical jargon.\\\"\\n\\n---\\n\\n### 3. Implementation (LangChain Style)\\nIf you are a developer, prompt templates are often handled programmatically. In the **LangChain** framework, it looks like this:\\n\\n```python\\nfrom langchain.prompts import PromptTemplate\\n\\ntemplate = \\\"I am traveling to {location}. What are the top {number} things to do there?\\\"\\n\\nprompt = PromptTemplate(\\n    input_variables=[\\\"location\\\", \\\"number\\\"],\\n    template=template,\\n)\\n\\n# This generates: \\\"I am traveling to Tokyo. What are the top 3 things to do there?\\\"\\nprint(prompt.format(location=\\\"Tokyo\\\", number=\\\"3\\\"))\\n```\\n\\n---\\n\\n### 4. Advanced Template Techniques\\n\\n*   **Few-Shot Prompting:** Include examples within your template to show the AI exactly how to respond.\\n    *   *Template:* \\\"Classify the sentiment of these reviews. \\n    Review: The food was great! -> Sentiment: Positive\\n    Review: I hated the service. -> Sentiment: Negative\\n    Review: {user_review} -> Sentiment: \\\"\\n*   **Delimiters:** Use symbols like `###`, `\\\"\\\"\\\"`, or `---` to separate instructions from the content variables. This helps the AI understand where the data starts and the instructions end.\\n*   **Chain-of-Thought:** Add a line like `\\\"Let's think step-by-step\\\"` to the end of your template to improve the AI's reasoning capabilities for complex tasks.\\n\\n---\\n\\n### 5. Why Use Prompt Templates?\\n1.  **Consistency:** Ensures the AI responds in the same format every time.\\n2.  **Efficiency:** You don't have to rewrite the \\\"Role\\\" and \\\"Context\\\" every time you want to perform a task.\\n3.  **Scalability:** Allows you to automate AI tasks across hundreds of different inputs (e.g., generating product descriptions for an entire catalog).\\n4.  **Optimization:** It's easier to \\\"version control\\\" your prompts. If a prompt isn't working, you tweak the template once rather than fixing individual prompts.\\n\\n**Would you like me to create a specific template for a task you're working on?**\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"index\": 0\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 4,\n",
              "        \"candidates_token_count\": 859,\n",
              "        \"total_token_count\": 1362\n",
              "      },\n",
              "      \"model_version\": \"gemini-3-flash-preview\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=model.generate_content('prompt_templates')\n",
        "response.text.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "PjXGlfaGQq_C",
        "outputId": "d5bf8577-8d61-4ea1-efdc-21c28ef951c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'**Prompt templates** are predefined structures used to generate consistent, high-quality inputs for Large Language Models (LLMs) like ChatGPT, Claude, or Llama. They use **placeholders** (variables) that can be swapped out with specific data depending on the task.\\n\\nThink of a prompt template as a \"Mad Libs\" for AI.\\n\\n---\\n\\n### 1. Basic Structure of a Template\\nA professional prompt template usually includes these components:\\n*   **Role:** Who the AI should act as (e.g., \"Senior Python Developer\").\\n*   **Context:** Background information or data.\\n*   **Task:** The specific instruction.\\n*   **Constraints:** What the AI should *not* do.\\n*   **Output Format:** How the result should look (JSON, Markdown, Table).\\n\\n---\\n\\n### 2. Examples of Common Templates\\n\\n#### A. Content Summarization\\n> \"Act as a professional editor. Summarize the following text for a **{target_audience}**. Keep the summary under **{word_count}** words and focus on **{key_theme}**.\\n> \\n> **Text:** {text_to_summarize}\\n> \\n> **Summary:**\"\\n\\n#### B. Code Generation\\n> \"You are an expert **{language}** developer. Write a function to **{task}**. \\n> Ensure the code is:\\n> 1. Optimized for performance.\\n> 2. Documented with comments.\\n> 3. Includes error handling.\\n>\\n> **Output format:** Provide only the code block and a brief explanation.\"\\n\\n#### C. Customer Support Reply\\n> \"You are a customer success agent at a company that sells **{product}**. A customer is writing to complain about **{issue}**. \\n> Tone: **{tone}** (e.g., empathetic, professional).\\n> \\n> **Customer Email:** {customer_email}\\n> \\n> **Response:**\"\\n\\n---\\n\\n### 3. Advanced Techniques\\n\\n#### Few-Shot Prompting\\nInclude examples inside the template to \"teach\" the AI the pattern you want.\\n> \"Extract the names of companies and their stock tickers from the text.\\n> \\n> Input: Apple Inc reported a rise in revenue.\\n> Output: Apple Inc (AAPL)\\n> \\n> Input: Microsoft saw growth in cloud services.\\n> Output: Microsoft (MSFT)\\n> \\n> Input: **{input_text}**\\n> Output:\"\\n\\n#### Chain-of-Thought (CoT)\\nForce the AI to explain its reasoning before giving the final answer.\\n> \"Solve the following math problem: **{problem}**\\n> \\n> **Step 1:** Break down the variables.\\n> **Step 2:** Show the calculation steps.\\n> **Step 3:** Provide the final answer.\"\\n\\n---\\n\\n### 4. Frameworks for Managing Templates\\nIf you are building an application, you shouldn\\'t hard-code strings. Use these tools:\\n\\n*   **LangChain:** Uses a `PromptTemplate` class.\\n    ```python\\n    from langchain.prompts import PromptTemplate\\n    template = \"Explain {topic} to a 5-year-old.\"\\n    prompt = PromptTemplate.from_template(template)\\n    print(prompt.format(topic=\"quantum physics\"))\\n    ```\\n*   **LlamaIndex:** Similar to LangChain, focused on data retrieval.\\n*   **Promptfoo / LangSmith:** Tools for testing and versioning your templates to see which version performs best.\\n\\n---\\n\\n### 5. Best Practices for Writing Templates\\n1.  **Use Delimiters:** Use `###`, `\"\"\"`, or `---` to separate the instructions from the user data. This prevents \"Prompt Injection\" (where the user\\'s data tries to hijack the instructions).\\n2.  **Be Specific:** Instead of \"Write a blog post,\" use \"Write a 500-word SEO-optimized blog post about gardening in small apartments.\"\\n3.  **Positive Constraints:** AI follows \"Do this\" better than \"Don\\'t do that.\" Instead of \"Don\\'t be wordy,\" use \"Be concise and use bullet points.\"\\n4.  **Define Output:** If you need to parse the data later, always ask for **JSON** or **CSV** format.\\n\\n**Would you like a specific template for a certain task (e.g., marketing, coding, or data analysis)?**'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibDJOZNZTrtP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}